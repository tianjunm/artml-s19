{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.dataset import MNIST\n",
    "from cleverhans.utils import AccuracyReport\n",
    "from cleverhans.utils_keras import cnn_model\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "NB_EPOCHS = 6\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = .001\n",
    "\n",
    "\n",
    "def mnist_tutorial(train_start=0, train_end=60000, test_start=0,\n",
    "                   test_end=10000, nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                   learning_rate=LEARNING_RATE, testing=False,\n",
    "                   label_smoothing=0.1):\n",
    "  \"\"\"\n",
    "  MNIST CleverHans tutorial\n",
    "  :param train_start: index of first training set example\n",
    "  :param train_end: index of last training set example\n",
    "  :param test_start: index of first test set example\n",
    "  :param test_end: index of last test set example\n",
    "  :param nb_epochs: number of epochs to train model\n",
    "  :param batch_size: size of training batches\n",
    "  :param learning_rate: learning rate for training\n",
    "  :param testing: if true, training error is calculated\n",
    "  :param label_smoothing: float, amount of label smoothing for cross entropy\n",
    "  :return: an AccuracyReport object\n",
    "  \"\"\"\n",
    "\n",
    "  # Object used to keep track of (and return) key accuracies\n",
    "  report = AccuracyReport()\n",
    "\n",
    "  # Set TF random seed to improve reproducibility\n",
    "  tf.set_random_seed(1234)\n",
    "  # Force TensorFlow to use single thread to improve reproducibility\n",
    "  config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                          inter_op_parallelism_threads=1)\n",
    "\n",
    "  if keras.backend.image_data_format() != 'channels_last':\n",
    "    raise NotImplementedError(\"this tutorial requires keras to be configured to channels_last format\")\n",
    "\n",
    "  # Create TF session and set as Keras backend session\n",
    "  sess = tf.Session(config=config)\n",
    "  keras.backend.set_session(sess)\n",
    "\n",
    "  # Get MNIST test data\n",
    "  mnist = MNIST(train_start=train_start, train_end=train_end,\n",
    "                test_start=test_start, test_end=test_end)\n",
    "  x_train, y_train = mnist.get_set('train')\n",
    "  x_test, y_test = mnist.get_set('test')\n",
    "\n",
    "  # Obtain Image Parameters\n",
    "  img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
    "  nb_classes = y_train.shape[1]\n",
    "\n",
    "  # Label smoothing\n",
    "  y_train -= label_smoothing * (y_train - 1. / nb_classes)\n",
    "\n",
    "  # Define Keras model\n",
    "  model = cnn_model(img_rows=img_rows, img_cols=img_cols,\n",
    "                    channels=nchannels, nb_filters=64,\n",
    "                    nb_classes=nb_classes)\n",
    "  print(\"Defined Keras model.\")\n",
    "\n",
    "  # To be able to call the model in the custom loss, we need to call it once\n",
    "  # before, see https://github.com/tensorflow/tensorflow/issues/23769\n",
    "#   model(model.input)\n",
    "\n",
    "  # Initialize the Fast Gradient Sign Method (FGSM) attack object\n",
    "  wrap = KerasModelWrapper(model)\n",
    "  fgsm = FastGradientMethod(wrap, sess=sess)\n",
    "  fgsm_params = {'eps': 0.3,\n",
    "                 'clip_min': 0.,\n",
    "                 'clip_max': 1.}\n",
    "\n",
    "  adv_acc_metric = get_adversarial_acc_metric(model, fgsm, fgsm_params)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy', adv_acc_metric]\n",
    "  )\n",
    "\n",
    "  # Train an MNIST model\n",
    "  model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=nb_epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=2)\n",
    "\n",
    "  # Evaluate the accuracy on legitimate and adversarial test examples\n",
    "  _, acc, adv_acc = model.evaluate(x_test, y_test,\n",
    "                                   batch_size=batch_size,\n",
    "                                   verbose=0)\n",
    "  report.clean_train_clean_eval = acc\n",
    "  report.clean_train_adv_eval = adv_acc\n",
    "  print('Test accuracy on legitimate examples: %0.4f' % acc)\n",
    "  print('Test accuracy on adversarial examples: %0.4f\\n' % adv_acc)\n",
    "\n",
    "  # Calculate training error\n",
    "  if testing:\n",
    "    _, train_acc, train_adv_acc = model.evaluate(x_train, y_train,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 verbose=0)\n",
    "    report.train_clean_train_clean_eval = train_acc\n",
    "    report.train_clean_train_adv_eval = train_adv_acc\n",
    "\n",
    "  print(\"Repeating the process, using adversarial training\")\n",
    "  # Redefine Keras model\n",
    "  model_2 = cnn_model(img_rows=img_rows, img_cols=img_cols,\n",
    "                      channels=nchannels, nb_filters=64,\n",
    "                      nb_classes=nb_classes)\n",
    "  wrap_2 = KerasModelWrapper(model_2)\n",
    "  fgsm_2 = FastGradientMethod(wrap_2, sess=sess)\n",
    "\n",
    "  # Use a loss function based on legitimate and adversarial examples\n",
    "  adv_loss_2 = get_adversarial_loss(model_2, fgsm_2, fgsm_params)\n",
    "  adv_acc_metric_2 = get_adversarial_acc_metric(model_2, fgsm_2, fgsm_params)\n",
    "  model_2.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate),\n",
    "      loss=adv_loss_2,\n",
    "      metrics=['accuracy', adv_acc_metric_2]\n",
    "  )\n",
    "\n",
    "  # Train an MNIST model\n",
    "  model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              verbose=2)\n",
    "\n",
    "  # Evaluate the accuracy on legitimate and adversarial test examples\n",
    "  _, acc, adv_acc = model_2.evaluate(x_test, y_test,\n",
    "                                     batch_size=batch_size,\n",
    "                                     verbose=0)\n",
    "  report.adv_train_clean_eval = acc\n",
    "  report.adv_train_adv_eval = adv_acc\n",
    "  print('Test accuracy on legitimate examples: %0.4f' % acc)\n",
    "  print('Test accuracy on adversarial examples: %0.4f\\n' % adv_acc)\n",
    "\n",
    "  # Calculate training error\n",
    "  if testing:\n",
    "    _, train_acc, train_adv_acc = model_2.evaluate(x_train, y_train,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   verbose=0)\n",
    "    report.train_adv_train_clean_eval = train_acc\n",
    "    report.train_adv_train_adv_eval = train_adv_acc\n",
    "\n",
    "  return report\n",
    "\n",
    "\n",
    "def get_adversarial_acc_metric(model, fgsm, fgsm_params):\n",
    "  def adv_acc(y, _):\n",
    "    # Generate adversarial examples\n",
    "    x_adv = fgsm.generate(model.input, **fgsm_params)\n",
    "    # Consider the attack to be constant\n",
    "    x_adv = tf.stop_gradient(x_adv)\n",
    "\n",
    "    # Accuracy on the adversarial examples\n",
    "    preds_adv = model(x_adv)\n",
    "    return keras.metrics.categorical_accuracy(y, preds_adv)\n",
    "\n",
    "  return adv_acc\n",
    "\n",
    "\n",
    "def get_adversarial_loss(model, fgsm, fgsm_params):\n",
    "  def adv_loss(y, preds):\n",
    "    # Cross-entropy on the legitimate examples\n",
    "    cross_ent = keras.losses.categorical_crossentropy(y, preds)\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    x_adv = fgsm.generate(model.input, **fgsm_params)\n",
    "    # Consider the attack to be constant\n",
    "    x_adv = tf.stop_gradient(x_adv)\n",
    "\n",
    "    # Cross-entropy on the adversarial examples\n",
    "    preds_adv = model(x_adv)\n",
    "    cross_ent_adv = keras.losses.categorical_crossentropy(y, preds_adv)\n",
    "\n",
    "    return 0.5 * cross_ent + 0.5 * cross_ent_adv\n",
    "\n",
    "  return adv_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined Keras model.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 27s - loss: 1.1831 - acc: 0.8071 - adv_acc: 0.0595 - val_loss: 0.4618 - val_acc: 0.9071 - val_adv_acc: 0.0459\n",
      "Epoch 2/10\n",
      " - 24s - loss: 0.8204 - acc: 0.9100 - adv_acc: 0.0441 - val_loss: 0.3687 - val_acc: 0.9297 - val_adv_acc: 0.0361\n",
      "Epoch 3/10\n",
      " - 24s - loss: 0.7548 - acc: 0.9315 - adv_acc: 0.0348 - val_loss: 0.3103 - val_acc: 0.9454 - val_adv_acc: 0.0295\n",
      "Epoch 4/10\n",
      " - 24s - loss: 0.7099 - acc: 0.9466 - adv_acc: 0.0277 - val_loss: 0.2727 - val_acc: 0.9572 - val_adv_acc: 0.0217\n",
      "Epoch 5/10\n",
      " - 24s - loss: 0.6778 - acc: 0.9571 - adv_acc: 0.0229 - val_loss: 0.2488 - val_acc: 0.9645 - val_adv_acc: 0.0188\n",
      "Epoch 6/10\n",
      " - 24s - loss: 0.6560 - acc: 0.9635 - adv_acc: 0.0194 - val_loss: 0.2302 - val_acc: 0.9690 - val_adv_acc: 0.0165\n",
      "Epoch 7/10\n",
      " - 24s - loss: 0.6399 - acc: 0.9684 - adv_acc: 0.0170 - val_loss: 0.2101 - val_acc: 0.9728 - val_adv_acc: 0.0151\n",
      "Epoch 8/10\n",
      " - 24s - loss: 0.6279 - acc: 0.9720 - adv_acc: 0.0157 - val_loss: 0.2008 - val_acc: 0.9761 - val_adv_acc: 0.0131\n",
      "Epoch 9/10\n",
      " - 24s - loss: 0.6181 - acc: 0.9745 - adv_acc: 0.0142 - val_loss: 0.1963 - val_acc: 0.9779 - val_adv_acc: 0.0126\n",
      "Epoch 10/10\n",
      " - 24s - loss: 0.6100 - acc: 0.9766 - adv_acc: 0.0134 - val_loss: 0.1911 - val_acc: 0.9800 - val_adv_acc: 0.0108\n",
      "Test accuracy on legitimate examples: 0.9800\n",
      "Test accuracy on adversarial examples: 0.0108\n",
      "\n",
      "Repeating the process, using adversarial training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Layer sequential_4 has multiple inbound nodes, hence the notion of \"layer input\" is ill-defined. Use `get_input_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2f2b116f0ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist_tutorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-39bb7c5becaa>\u001b[0m in \u001b[0;36mmnist_tutorial\u001b[0;34m(train_start, train_end, test_start, test_end, nb_epochs, batch_size, learning_rate, testing, label_smoothing)\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madv_loss_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_acc_metric_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 342\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-39bb7c5becaa>\u001b[0m in \u001b[0;36madv_loss\u001b[0;34m(y, preds)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Generate adversarial examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfgsm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Consider the attack to be constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 784\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer input\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer sequential_4 has multiple inbound nodes, hence the notion of \"layer input\" is ill-defined. Use `get_input_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "mnist_tutorial(nb_epochs=10, batch_size=32, learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
